{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;color:orange\">Domain4 - Machine Learning Implementation and Operations</h1>\n",
    "<p style=\"text-align:right;\">&copy; FebaTech</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker:\n",
    "\n",
    "Docker is a tool designed to make it easier to create, deploy, and run applications by using containers. Containers allow a developer to package up an application with all of the parts it needs, such as libraries and other dependencies, and deploy it as one package. By doing so, thanks to the container, the developer can rest assured that the application will run on any other Linux machine regardless of any customized settings that machine might have that could differ from the machine used for writing and testing the code.\n",
    "\n",
    "# Sagemaker:\n",
    "<img src=\"Assets/Amazon-SageMaker_light-bg@4x.png\" alt=\"SageMaker\"></img>\n",
    "- All models in sagemaker are hosted in docker containers\n",
    "- These Docker containers are created from Docker images that are in Amazon ECR\n",
    "- You can import a docker image or create your own image from scratch\n",
    "- You can test out multiple models on live traffic using Production Variants. Variant Weights tell SageMaker how to distribute traffic among them\n",
    "- For real time inference on edge devices a cloud round trip doesnâ€™t make any sense hence Amazon SageMaker Neo which enables developers to train machine learning models once and run them anywhere in the cloud and at the edge. Amazon SageMaker Neo optimizes models to run up to twice as fast, with less than a tenth of the memory footprint, with no loss in accuracy. One can deploy Neo to an HTTPS endpoint hosted by EC2 instances or with AWS IoT Greengrass\n",
    "\n",
    "# Security\n",
    "- Identity and Access Management\n",
    "- Multi Factor Authentication\n",
    "- SSL/TLS while connecting\n",
    "- CloudTrail to log API and user activity and CloudWatch to monitor and alarm\n",
    "- Encryption\n",
    "- Inter node traffic communication encryption when sagemaker is using multiple nodes\n",
    "- Private Virtual private cloud(VPC) can be used for more security but one needs to create s3 VPC endpoints to get/put data to s3\n",
    "\n",
    "# Resources Management\n",
    "- Choosing instances based on your need like p3,p2 which has GPU and are bit pricey\n",
    "- Spot Instances(Amazon EC2 Spot instance to run training jobs instead of on-demand instances). These can be interrupted hence you need to save the progress (Checkpoints) so that you can carry on from where you left.\n",
    "- Amazon Elastic Inference allows you to attach low-cost GPU-powered acceleration to Amazon EC2 and Sagemaker instances to reduce the cost of running deep learning inference by up to 75%. Amazon Elastic Inference supports TensorFlow, Apache MXNet, PyTorch and ONNX models. These can be added alongside CPU instances.\n",
    "- With Automatic Scaling one can set up a scaling policy to define target metrics, min/max capacity, cooldown periods. It is done with CloudWatch.\n",
    "- SageMaker automatically attempts to distribute instances across availability zones, but you need more than one instance for this to work, even if your model does not require more than one instance it is better to have 2 instances in case if one gets crashed another one can provide service.\n",
    "- Inference Pipelines can be used for linear sequencing of 2-5 containers where each container is used for different processes like preprocessing, training, deployment etc ...\n",
    "\n",
    "\n",
    "**The notebook which we are going to use is provided as cancer-demo-sagemaker.ipynb, cancerDemo.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this tutorial we will Download data from GitHub, Store it in S3; Train,Deploy model on EC2 Instances with SagemMaker. The problem statement is to Predict IDC in Breast Cancer Histology Images with transfer learning**\n",
    "<img src=\"Assets/deploy_flow.png\" alt=\"FLow\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video alt=\"Glue_Crawler\" height=500px width=900px controls>\n",
       "        <source src=\"Assets/domain4.mp4\" type=\"video/mp4\">\n",
       "    </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "    <video alt=\"Glue_Crawler\" height=500px width=900px controls>\n",
    "        <source src=\"Assets/domain4.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
